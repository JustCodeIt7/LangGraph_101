{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph: Multi-Agent Parallel Processing with Different LLMs\n",
    "\n",
    "-  **Definition and Purpose**: Parallelization allows Large Language Models (LLMs) to handle a task simultaneously, merging their outputs to increase efficiency or improve result quality.\n",
    "-  **Two Main Variations**:\n",
    "      - \"Sectioning\" (splitting a task into independent subtasks for parallel execution)\n",
    "      - \"Voting\" (executing the same task multiple times to obtain diverse outputs).\n",
    "-  **Optimal Use Cases**:\n",
    "     - Speeds up tasks through parallel subtask execution.\n",
    "     - Provides higher confidence by gathering multiple perspectives, especially for complex tasks where LLMs can focus on specific aspects.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "-  **Multi-Agent Architecture**: Rather than relying on a single agent, a team of specialized agents collaborates. Each agent has a unique role and is powered by a carefully chosen LLM.\n",
    "-  **Parallel Execution**: These agents operate concurrently. The workflow starts at a common point, branches out to individual agents, and converges at the final node upon completion.\n",
    "-  **LLM Specialization**: Different OpenAI models are matched to specific tasks:\n",
    "     - `gpt-4o`: A powerful model for generating insightful and relevant questions.\n",
    "     - `gpt-4o-mini`: A cost-effective, creative model ideal for crafting jokes.\n",
    "     - `gpt-3.5-turbo`: A fast, balanced model for identifying related topics.\n",
    "-  **State Management**: The `AgentState` TypedDict facilitates information sharing among agents. It serves as a shared memory space where each agent can access the initial topic and store its results.\n",
    "\n",
    "## 1. Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from rich import print\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "if langchain_api_key:\n",
    "    os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Graph State\n",
    "\n",
    "- `topic`: The input topic that the agents will work on.\n",
    "- `questions`: A list to store the questions generated by the `question_agent`.\n",
    "- `jokes`: A list to store the jokes generated by the `joke_agent`.\n",
    "- `related_topics`: A list to store related topics from the `related_topics_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    questions: List[str]\n",
    "    jokes: List[str]\n",
    "    related_topics: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing Specialized LLMs\n",
    "\n",
    "- Initialize three specialized LLMs for specific tasks.  \n",
    "- Each model is chosen for its suitability for a specific task. This is a key aspect of the multi-agent design pattern, allowing for optimization of both performance and cost.\n",
    "\n",
    "### LLMs Used:\n",
    "- `llm_questions` (`gpt-4o`): Chosen for its strong reasoning and generation capabilities to produce high-quality, relevant questions.\n",
    "- `llm_jokes` (`gpt-4o-mini`): A more creative and less expensive model, ideal for the lighthearted task of generating humor.\n",
    "- `llm_related` (`gpt-3.5-turbo`): A fast and balanced model, well-suited for identifying connections and related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_questions = ChatOpenAI(model='gpt-4o', temperature=0.1)  # Most capable for thoughtful questions\n",
    "llm_jokes = ChatOpenAI(model='gpt-4o-mini', temperature=0.8)  # Creative for humor\n",
    "llm_related = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.3)  # Balanced for topic connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Generation Chains\n",
    "\n",
    "We create a helper function `create_generation_chain` to build the processing chains for our agents. Each chain consists of a prompt, an LLM, and a JSON output parser.\n",
    "\n",
    "- **Prompt**: The `ChatPromptTemplate` guides the LLM on what to generate.\n",
    "- **LLM**: The language model that processes the prompt.\n",
    "- **Parser**: The `JsonOutputParser` ensures the output is in a structured JSON format, making it easy to integrate back into our `AgentState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_chain(llm, prompt_template_str: str):\n",
    "    \"\"\"Creates a chain for generating content based on a topic.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "    parser = JsonOutputParser()\n",
    "    return prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this helper function to create a specific chain for each agent, each with its own prompt and specialized LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_chain = create_generation_chain(\n",
    "    llm_questions,\n",
    "    'Generate 5 questions about the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")\n",
    "joke_chain = create_generation_chain(\n",
    "    llm_jokes,\n",
    "    'Generate 3 jokes about the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")\n",
    "related_topics_chain = create_generation_chain(\n",
    "    llm_related,\n",
    "    'Generate 4 related topics for the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining the Agent Nodes\n",
    "\n",
    "Each agent is defined as a node in the graph. These functions take the current state as input, invoke their respective generation chains, and return the results to be merged back into the state.\n",
    "\n",
    "- `question_agent`: Generates questions using `gpt-4o`.\n",
    "- `joke_agent`: Generates jokes using `gpt-4o-mini`.\n",
    "- `related_topics_agent`: Generates related topics using `gpt-3.5-turbo`.\n",
    "\n",
    "Each agent includes error handling to gracefully manage any issues during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_agent(state: AgentState):\n",
    "    \"\"\"Generates questions based on the topic using GPT-4o.\"\"\"\n",
    "    print('---GENERATING QUESTIONS (GPT-4o)---')\n",
    "    try:\n",
    "        result = question_chain.invoke({'topic': state['topic']})\n",
    "        return {'questions': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in question_agent: {e}')\n",
    "        return {'questions': [f'Error generating questions: {str(e)}']}\n",
    "\n",
    "def joke_agent(state: AgentState):\n",
    "    \"\"\"Generates jokes based on the topic using GPT-4o-mini.\"\"\"\n",
    "    print('---GENERATING JOKES (GPT-4o-mini)---')\n",
    "    try:\n",
    "        result = joke_chain.invoke({'topic': state['topic']})\n",
    "        return {'jokes': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in joke_agent: {e}')\n",
    "        return {'jokes': [f'Error generating jokes: {str(e)}']}\n",
    "\n",
    "def related_topics_agent(state: AgentState):\n",
    "    \"\"\"Generates related topics based on the topic using GPT-3.5-turbo.\"\"\"\n",
    "    print('---GENERATING RELATED TOPICS (GPT-3.5-turbo)---')\n",
    "    try:\n",
    "        result = related_topics_chain.invoke({'topic': state['topic']})\n",
    "        return {'related_topics': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in related_topics_agent: {e}')\n",
    "        return {'related_topics': [f'Error generating related topics: {str(e)}']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building and Compiling the Graph\n",
    "\n",
    "Now we assemble the graph using `StateGraph`.\n",
    "\n",
    "1.  **Add Nodes**: We add each agent function as a node in the graph.\n",
    "2.  **Set Entry Point**: A key feature for parallel execution is setting multiple entry points. By calling `set_entry_point` for each agent, we instruct LangGraph to run them all concurrently as soon as the graph is invoked.\n",
    "3.  **Add Edges**: We define the graph's flow. Since all agents run in parallel and the process finishes after they are done, we add an edge from each agent node directly to `END`.\n",
    "4.  **Compile**: Finally, we compile the workflow into a runnable application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for each agent\n",
    "workflow.add_node('question_agent', question_agent)\n",
    "workflow.add_node('joke_agent', joke_agent)\n",
    "workflow.add_node('related_topics_agent', related_topics_agent)\n",
    "\n",
    "# The entry point is the topic, which then triggers the parallel agents\n",
    "workflow.set_entry_point('question_agent')\n",
    "workflow.set_entry_point('joke_agent')\n",
    "workflow.set_entry_point('related_topics_agent')\n",
    "\n",
    "# All parallel agents lead to the end\n",
    "workflow.add_edge('question_agent', END)\n",
    "workflow.add_edge('joke_agent', END)\n",
    "workflow.add_edge('related_topics_agent', END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Graph\n",
    "\n",
    "Here is a visual representation of our parallel agent graph. As you can see, the graph starts and immediately branches out to the three agents, which run in parallel before reaching the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # Generate the Mermaid diagram and save it as a PNG\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error generating graph visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running the Graph\n",
    "\n",
    "With the graph compiled, we can now run it. We'll test it with a few different topics to see how the agents perform.\n",
    "\n",
    "For each topic:\n",
    "1.  We define an `initial_state` dictionary containing the topic.\n",
    "2.  We `invoke` the compiled app with this state.\n",
    "3.  The graph executes the three agents in parallel.\n",
    "4.  The final state, containing the results from all agents, is returned and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🚀 LangGraph Multi-Agent Parallel Processing with Different LLMs')\n",
    "print('=' * 65)\n",
    "print('🤖 Question Agent: GPT-4o (Most capable for thoughtful questions)')\n",
    "print('😂 Joke Agent: GPT-4o-mini (Creative and cost-effective for humor)')\n",
    "print('🔗 Related Topics Agent: GPT-3.5-turbo (Balanced for connections)')\n",
    "print('=' * 65)\n",
    "\n",
    "# Test with multiple topics\n",
    "test_topics = ['Artificial Intelligence', 'Climate Change', 'Space Exploration']\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f'\\n🎯 Processing Topic: {topic}')\n",
    "    print('⚡ Running parallel agents with different LLMs...')\n",
    "\n",
    "    initial_state = {'topic': topic, 'questions': [], 'jokes': [], 'related_topics': []}\n",
    "\n",
    "    try:\n",
    "        final_state = app.invoke(initial_state)\n",
    "\n",
    "        print(f'\\n📊 RESULTS FOR: {final_state[\"topic\"].upper()}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        print(f'\\n🤔 QUESTIONS (Generated by GPT-4o):')\n",
    "        for i, q in enumerate(final_state['questions'], 1):\n",
    "            print(f'  {i}. {q}')\n",
    "\n",
    "        print(f'\\n😂 JOKES (Generated by GPT-4o-mini):')\n",
    "        for i, j in enumerate(final_state['jokes'], 1):\n",
    "            print(f'  {i}. {j}')\n",
    "\n",
    "        print(f'\\n🔗 RELATED TOPICS (Generated by GPT-3.5-turbo):')\n",
    "        for i, t in enumerate(final_state['related_topics'], 1):\n",
    "            print(f'  {i}. {t}')\n",
    "\n",
    "        print('\\n' + '=' * 65)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'❌ Error processing topic \"{topic}\": {str(e)}')\n",
    "\n",
    "print('\\n✅ Demo completed! Each agent used a different LLM model.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
