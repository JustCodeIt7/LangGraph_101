{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Orchestration Agent: Report Generator\n",
        "\n",
        "This Jupyter notebook demonstrates an orchestration agent utilizing LangGraph to generate a structured report on a specified topic. The agent orchestrates multiple steps, including planning report sections, writing content for each section, and compiling the final report.\n",
        "\n",
        "The workflow employs a `StateGraph` to manage shared state among nodes (agents). This example illustrates sequential orchestration, with an optional asynchronous parallel version for section writing.\n",
        "\n",
        "**Key Concepts:**\n",
        "- Shared state with `TypedDict`\n",
        "- LLM-powered agents for planning, writing, and compiling\n",
        "- Graph construction with nodes and edges\n",
        "- Synchronous and asynchronous execution\n",
        "\n",
        "Ensure you have an `OPENROUTER_API_KEY` environment variable set for the LLM to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and State Definition\n",
        "\n",
        "We start by importing necessary libraries from LangGraph, LangChain, and standard Python modules. The `ReportState` TypedDict defines the shared state that flows through the graph:\n",
        "- `topic`: The main subject of the report.\n",
        "- `sections`: List of section titles planned for the report.\n",
        "- `section_drafts`: Dictionary mapping section titles to their generated content.\n",
        "- `final_report`: The compiled markdown report.\n",
        "\n",
        "This state ensures all agents can access and update relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c453c26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports span typing hints, LangGraph orchestration, and LLM tooling.\n",
        "from typing import TypedDict, List, Dict, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import asyncio\n",
        "from rich import print\n",
        "import os\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# TypedDict keeps the mutable LangGraph state structured and type-checked.\n",
        "class ReportState(TypedDict):\n",
        "    topic: str\n",
        "    sections: List[str]  # This will be the list of section titles\n",
        "    section_drafts: Dict[str, str]  # Maps section titles to their content\n",
        "    final_report: str\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733a42c0",
      "metadata": {},
      "source": [
        "## 2. LLM Initialization\n",
        "\n",
        "Initialize the language model using `ChatOpenAI` configured for OpenRouter. This LLM will power all agents (planner, writer, compiler). Parameters like `temperature=0.7` control creativity, and `max_tokens=250` limits response length per call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3d05de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the shared LLM that will power every agent in the workflow.\n",
        "# llm = ChatOpenAI(\n",
        "#     model='qwen/qwen3-30b-a3b-instruct-2507',  # OpenRouter-hosted model with strong reasoning skills.\n",
        "#     base_url='https://openrouter.ai/api/v1',   # Direct requests through the OpenRouter endpoint.\n",
        "#     temperature=0.7,                           # Allow moderate creativity for nuanced sections.\n",
        "#     max_tokens=250,                            # Keep each response concise to control latency.\n",
        "#     api_key=os.getenv('OPENROUTER_API_KEY')    # Pull the API key from environment variables.\n",
        "# )\n",
        "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
        "llm = ChatOllama(model='llama3.2', base_url=OLLAMA_BASE_URL, temperature=0.7, max_tokens=400)\n",
        "# llm = ChatOllama(model='llama3.2:1b', base_url=OLLAMA_BASE_URL, temperature=0.7, max_tokens=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a91f86",
      "metadata": {},
      "source": [
        "## 3. Planner Agent\n",
        "\n",
        "The `planner_agent` is the first node in the graph. It takes the report topic and uses the LLM to generate 3-4 logical section titles. The prompt instructs the LLM to output only titles, one per line, without numbering.\n",
        "\n",
        "This agent updates the state with the `sections` list and initializes an empty `section_drafts` dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a88e757",
      "metadata": {},
      "outputs": [],
      "source": [
        "def planner_agent(state: ReportState) -> ReportState:\n",
        "    \"\"\"Plans the structure of the report by defining sections\"\"\"\n",
        "    # Pull the topic from the shared state dictionary supplied by LangGraph.\n",
        "    topic = state['topic']\n",
        "\n",
        "    # Build a targeted prompt so the LLM returns only section headings.\n",
        "    planning_prompt = f\"\"\"\n",
        "    You are a report planning expert. Given the topic: \"{topic}\"\n",
        "    \n",
        "    Create a logical outline with 3-4 main sections for a comprehensive report.\n",
        "    Return only the section titles, one per line, without numbering.\n",
        "    \n",
        "    Example format:\n",
        "    Introduction and Background\n",
        "    Current State Analysis\n",
        "    Future Implications\n",
        "    Conclusion and Recommendations\n",
        "    \"\"\"\n",
        "\n",
        "    # Ask the LLM for candidate section titles based on the topic.\n",
        "    response = llm.invoke([HumanMessage(content=planning_prompt)])\n",
        "    # Split the multiline LLM response into a clean list of section names.\n",
        "    sections = [line.strip() for line in response.content.strip().split('\\n') if line.strip()]\n",
        "\n",
        "    # Merge the new sections into state without mutating the original object.\n",
        "    return {**state, 'sections': sections, 'section_drafts': {}}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170f250a",
      "metadata": {},
      "source": [
        "## 4. Section Writer Helper\n",
        "\n",
        "The `write_section` helper function generates content for a single section. It classifies the section type (e.g., introductory vs. strategic) to adjust the prompt's focus and tone. The LLM is prompted to write 2-3 paragraphs without including the title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7744235e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_section(section_title: str, topic: str) -> str:\n",
        "    \"\"\"Helper function to write a single section\"\"\"\n",
        "    # Simple heuristics tune the focus and tone based on the section name.\n",
        "    if any(keyword in section_title.lower() for keyword in ['introduction', 'background', 'analysis', 'current']):\n",
        "        focus = 'research-backed information and analysis'\n",
        "        tone = 'professional and factual'\n",
        "    else:\n",
        "        focus = 'strategic insights and actionable recommendations'\n",
        "        tone = 'forward-thinking and suggestive'\n",
        "\n",
        "    # Compose instructions that tell the LLM how to draft the section body.\n",
        "    writer_prompt = f\"\"\"\n",
        "    You are an expert report writer. Write a detailed section for a report on \"{topic}\".\n",
        "    \n",
        "    Section to write: \"{section_title}\"\n",
        "    \n",
        "    Requirements:\n",
        "    - Write 2-3 substantial paragraphs.\n",
        "    - Focus on {focus}.\n",
        "    - Use a {tone}.\n",
        "    - Do not include the section title itself in your response, just the content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the section copy and trim whitespace before returning it.\n",
        "    response = llm.invoke([HumanMessage(content=writer_prompt)])\n",
        "    return response.content.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe62d7a",
      "metadata": {},
      "source": [
        "## 5. Writer Coordinator\n",
        "\n",
        "The `writer_coordinator` node orchestrates writing all sections by calling `write_section` for each title in sequence. It updates the state with the `section_drafts` dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7a9813",
      "metadata": {},
      "outputs": [],
      "source": [
        "def writer_coordinator(state: ReportState) -> ReportState:\n",
        "    \"\"\"Coordinates the writing of all sections\"\"\"\n",
        "    topic = state['topic']\n",
        "    sections = state['sections']\n",
        "    # Create a fresh mapping so we do not mutate the incoming state in place.\n",
        "    section_drafts = {}\n",
        "\n",
        "    # Iterate sequentially through the planned sections and draft each one.\n",
        "    for section_title in sections:\n",
        "        section_content = write_section(section_title, topic)\n",
        "        # Store the generated paragraphs under their corresponding title.\n",
        "        section_drafts[section_title] = section_content\n",
        "\n",
        "    return {**state, 'section_drafts': section_drafts}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fecf1c",
      "metadata": {},
      "source": [
        "## 6. Compiler Agent\n",
        "\n",
        "The `compiler_agent` finalizes the report by formatting the sections into a markdown string with the topic as the title and each section as a heading followed by its content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61207397",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compiler_agent(state: ReportState) -> ReportState:\n",
        "    \"\"\"Compiles all drafted sections into a final report\"\"\"\n",
        "    topic = state['topic']\n",
        "    section_drafts = state['section_drafts']\n",
        "    sections = state['sections']\n",
        "\n",
        "    # Start the markdown document with a descriptive heading for the topic.\n",
        "    final_report_content = f'# Report: {topic}\\n\\n'\n",
        "\n",
        "    # Walk through sections in outline order to preserve the planned flow.\n",
        "    for section_title in sections:\n",
        "        if section_title in section_drafts:\n",
        "            # Append each section as markdown headings plus the generated body.\n",
        "            final_report_content += f'## {section_title}\\n\\n{section_drafts[section_title]}\\n\\n'\n",
        "\n",
        "    return {**state, 'final_report': final_report_content}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8006871e",
      "metadata": {},
      "source": [
        "## 7. Async Versions for Parallel Writing\n",
        "\n",
        "To improve efficiency, we define async versions of the writer functions. `write_section_async` wraps the synchronous helper, and `writer_coordinator_async` uses `asyncio.gather` to write all sections in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d624ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Async version for true parallel execution\n",
        "async def write_section_async(section_title: str, topic: str) -> str:\n",
        "    \"\"\"Async helper function to write a single section\"\"\"\n",
        "    # Reuse the synchronous section writer inside an async-friendly wrapper.\n",
        "    return write_section(section_title, topic)\n",
        "\n",
        "\n",
        "async def writer_coordinator_async(state: ReportState) -> ReportState:\n",
        "    \"\"\"Async coordinator for parallel section writing\"\"\"\n",
        "    topic = state['topic']\n",
        "    sections = state['sections']\n",
        "\n",
        "    # Create tasks for parallel execution of each section draft.\n",
        "    tasks = [write_section_async(section, topic) for section in sections]\n",
        "\n",
        "    # Execute all tasks in parallel and collect their generated content.\n",
        "    section_contents = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Map the list of generated sections back to their original titles.\n",
        "    section_drafts = dict(zip(sections, section_contents))\n",
        "\n",
        "    return {**state, 'section_drafts': section_drafts}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c650949",
      "metadata": {},
      "source": [
        "## 8. Graph Construction (Synchronous)\n",
        "\n",
        "Build the `StateGraph` by adding nodes for each agent and defining edges for the sequential flow: planner → writer_coordinator → compiler → END.\n",
        "\n",
        "Compile the graph into an app for invocation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8004d04a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Graph Construction ---\n",
        "\n",
        "# Build the orchestration graph specifying each agent node and its edges.\n",
        "workflow = StateGraph(ReportState)\n",
        "\n",
        "# Add nodes that correspond to each stage of the reporting pipeline.\n",
        "workflow.add_node('planner', planner_agent)\n",
        "workflow.add_node('writer_coordinator', writer_coordinator)\n",
        "workflow.add_node('compiler', compiler_agent)\n",
        "\n",
        "# The planner kicks off the workflow by generating the report outline.\n",
        "workflow.set_entry_point('planner')\n",
        "\n",
        "# Connect nodes to define a simple linear progression.\n",
        "workflow.add_edge('planner', 'writer_coordinator')\n",
        "workflow.add_edge('writer_coordinator', 'compiler')\n",
        "workflow.add_edge('compiler', END)\n",
        "\n",
        "# Freeze the definition into an executable LangGraph app.\n",
        "app = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0b7d76",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Subgraph structure:')\n",
        "# Visualize the compiled graph when graphviz support is available.\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(app.get_graph().draw_mermaid_png()))\n",
        "# save to file\n",
        "\n",
        "app.get_graph().draw_mermaid_png(output_file_path='graph.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928a7d66",
      "metadata": {},
      "source": [
        "## 10. Execution Functions\n",
        "\n",
        "`generate_report` runs the synchronous graph to produce the final report. `generate_report_async` builds and runs an async version of the graph, using the parallel writer coordinator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ac36cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Execution Functions ---\n",
        "\n",
        "\n",
        "def generate_report(topic: str) -> str:\n",
        "    \"\"\"Generate a report using the sequential workflow.\"\"\"\n",
        "    # Seed the graph state with empty containers for sections and drafts.\n",
        "    initial_state = {'topic': topic, 'sections': [], 'section_drafts': {}, 'final_report': ''}\n",
        "    # debug=True prints each node transition for instructional purposes.\n",
        "    result = app.invoke(initial_state, debug=True)\n",
        "    return result['final_report']\n",
        "\n",
        "\n",
        "async def generate_report_async(topic: str) -> str:\n",
        "    \"\"\"Generate a report with parallel section writing.\"\"\"\n",
        "    # Create async workflow that swaps in the concurrent writer coordinator.\n",
        "    async_workflow = StateGraph(ReportState)\n",
        "    async_workflow.add_node('planner', planner_agent)\n",
        "    async_workflow.add_node('writer_coordinator', writer_coordinator_async)\n",
        "    async_workflow.add_node('compiler', compiler_agent)\n",
        "\n",
        "    async_workflow.set_entry_point('planner')\n",
        "    async_workflow.add_edge('planner', 'writer_coordinator')\n",
        "    async_workflow.add_edge('writer_coordinator', 'compiler')\n",
        "    async_workflow.add_edge('compiler', END)\n",
        "\n",
        "    # Compile the async workflow so it can be awaited.\n",
        "    async_app = async_workflow.compile()\n",
        "\n",
        "    initial_state = {'topic': topic, 'sections': [], 'section_drafts': {}, 'final_report': ''}\n",
        "    # Kick off the asynchronous execution pathway.\n",
        "    result = await async_app.ainvoke(initial_state)\n",
        "    return result['final_report']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2afa55e",
      "metadata": {},
      "source": [
        "## 11. Running the Example\n",
        "\n",
        "The main execution generates a sample report on 'The Impact of Artificial Intelligence on Healthcare' using the synchronous function. For async, uncomment and run with `asyncio.run`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ab5e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose a sample topic to demonstrate the orchestration pipeline.\n",
        "topic = 'The Impact of Artificial Intelligence on Healthcare'\n",
        "\n",
        "# Generate the report synchronously so we can inspect the output immediately.\n",
        "print(f'Generating report on: {topic}...')\n",
        "# report = generate_report(topic)\n",
        "# print(report)\n",
        "\n",
        "# For async version:\n",
        "# In a Jupyter notebook, which runs an event loop, use await instead of asyncio.run()\n",
        "report_async = await generate_report_async(topic)  # Run the parallel variant.\n",
        "print(report_async)\n",
        "\n",
        "# Run the example (uncomment to execute)\n",
        "# topic = 'The Impact of Artificial Intelligence on Healthcare'\n",
        "# report = generate_report(topic)\n",
        "# print(report)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
