{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Routing with LangGraph\n",
    "\n",
    "This notebook demonstrates how to build a multi-agent system that directs queries to specialized language models (LLMs) based on their content. We will use LangGraph to create a stateful graph that manages the routing logic.\n",
    "\n",
    "## Key Features of the Router\n",
    "\n",
    "-  **Router Functionality:** Enables an LLM to select a single step from multiple options.\n",
    "-  **Router Control:** Offers limited control, as the LLM makes one decision and generates specific output from predefined choices.\n",
    "\n",
    "## Overview of Graph Structure\n",
    "\n",
    "-  A code-specialized LLM\n",
    "-  A math-specialized LLM\n",
    "-  A creative-specialized LLM\n",
    "-  A general-purpose LLM\n",
    "\n",
    "## 1. Imports and State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.750063Z",
     "start_time": "2025-09-09T00:16:43.263437Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from rich import print\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.756012Z",
     "start_time": "2025-09-09T00:16:43.754028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the state structure\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    route: str\n",
    "    response: str\n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mock LLM Definitions\n",
    "\n",
    "To simulate a multi-agent environment without requiring actual API calls, we define several mock LLM functions. Each function represents a specialized agent and returns a formatted string indicating which LLM was called.\n",
    "\n",
    "In a real-world application, these would be replaced with actual calls to different LLM APIs or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.762304Z",
     "start_time": "2025-09-09T00:16:43.760267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mock LLM responses (replace with actual LLM calls)\n",
    "def code_llm(query: str) -> str:\n",
    "    \"\"\"Specialized LLM for coding questions\"\"\"\n",
    "    return f\"[CODE LLM] Here's a programming solution for: {query}\"\n",
    "\n",
    "def math_llm(query: str) -> str:\n",
    "    \"\"\"Specialized LLM for math questions\"\"\"\n",
    "    return f'[MATH LLM] Mathematical analysis of: {query}'\n",
    "\n",
    "def general_llm(query: str) -> str:\n",
    "    \"\"\"General purpose LLM\"\"\"\n",
    "    return f'[GENERAL LLM] General response to: {query}'\n",
    "\n",
    "def creative_llm(query: str) -> str:\n",
    "    \"\"\"Specialized LLM for creative tasks\"\"\"\n",
    "    return f'[CREATIVE LLM] Creative response to: {query}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Router Function\n",
    "\n",
    "The `route_query` function is the core of our routing logic. It inspects the user's query for specific keywords to determine the most appropriate LLM. \n",
    "\n",
    "It uses predefined lists of keywords for different domains (code, math, creative). If a keyword is found, it sets the `route` and `reasoning` fields in the agent's state. If no specific keywords are matched, it defaults to the 'general' route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.768358Z",
     "start_time": "2025-09-09T00:16:43.765918Z"
    }
   },
   "outputs": [],
   "source": [
    "def route_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyze query and determine which LLM to use.\"\"\"\n",
    "    # Normalize the query to lowercase for case-insensitive matching\n",
    "    query = state['query'].lower()\n",
    "\n",
    "    # Define keywords for identifying code-related queries\n",
    "    code_keywords = [\n",
    "        'python', 'javascript', 'code', 'function', 'programming', \n",
    "        'algorithm', 'debug', 'api', 'class', 'variable',\n",
    "    ]\n",
    "\n",
    "    # Define keywords for identifying math-related queries\n",
    "    math_keywords = [\n",
    "        'calculate', 'equation', 'math', 'formula', 'solve', \n",
    "        'probability', 'statistics', 'derivative', 'integral',\n",
    "    ]\n",
    "\n",
    "    # Define keywords for identifying creative writing queries\n",
    "    creative_keywords = ['story', 'poem', 'creative', 'write', 'imagine', 'fictional', 'character', 'plot', 'narrative']\n",
    "\n",
    "    # Determine the appropriate route by matching keywords in the query\n",
    "    if any(keyword in query for keyword in code_keywords):\n",
    "        route = 'code'\n",
    "        reasoning = 'Detected programming/coding related query'\n",
    "    # Check for math keywords only if no code keywords were found\n",
    "    elif any(keyword in query for keyword in math_keywords):\n",
    "        route = 'math'\n",
    "        reasoning = 'Detected mathematical/calculation query'\n",
    "    # Check for creative writing keywords only if no code or math keywords were found\n",
    "    elif any(keyword in query for keyword in creative_keywords):\n",
    "        route = 'creative'\n",
    "        reasoning = 'Detected creative writing query'\n",
    "    else:\n",
    "        # Default to the general model if no specific keywords are found\n",
    "        route = 'general'\n",
    "        reasoning = 'No specific specialization detected, using general LLM'\n",
    "\n",
    "    # Update the agent's state with the routing decision and justification\n",
    "    state['route'] = route\n",
    "    state['reasoning'] = reasoning\n",
    "    \n",
    "    # Return the modified state object\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.774005Z",
     "start_time": "2025-09-09T00:16:43.771237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced router: Use an LLM to classify the query into one of the target domains.\n",
    "# Falls back to keyword routing (defined below) if LLM unavailable or errors.\n",
    "\n",
    "def llm_route_query(state: AgentState) -> AgentState:\n",
    "    \n",
    "    query = state[\"query\"].strip()\n",
    "\n",
    "    # llm = ChatOpenAI(model=\"gpt-4.1-nano\", max_tokens=500)\n",
    "    llm = ChatOllama(model='llama3.2', temperature=0)\n",
    "    \n",
    "    system = SystemMessage(content=\"\"\"\n",
    "You are a routing classifier. Given a user query, respond ONLY with one token from this set:\n",
    "code | math | creative | general\n",
    "Definitions:\n",
    "- code: programming, software engineering, APIs, debugging, algorithms.\n",
    "- math: equations, calculus, probability, statistics, numeric problem solving.\n",
    "- creative: storytelling, poems, fiction, characters, plot, imaginative writing.\n",
    "- general: anything else (explanations, science, history, general knowledge).\n",
    "If ambiguous, choose the most plausible specialized category else 'general'.\n",
    "Return a JSON object: {\"route\": <one of above>, \"reasoning\": \"short explanation\"}.\n",
    "Strict JSON.\n",
    "\"\"\".strip())\n",
    "    human = HumanMessage(content=f\"Query: {query}\")\n",
    "\n",
    "    raw = llm.invoke([system, human])\n",
    "    text = raw.content if hasattr(raw, 'content') else str(raw)\n",
    "    # Attempt to parse JSON\n",
    "    route = \"general\"\n",
    "    reasoning = \"LLM routing fallback to general\"\n",
    "    import json\n",
    "    m = None\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        route = data.get(\"route\", route)\n",
    "        reasoning = data.get(\"reasoning\", reasoning)\n",
    "    except json.JSONDecodeError:\n",
    "        # Heuristic extraction\n",
    "        lowered = text.lower()\n",
    "        for candidate in [\"code\", \"math\", \"creative\", \"general\"]:\n",
    "            if candidate in lowered:\n",
    "                route = candidate\n",
    "                reasoning = f\"Heuristic parse from LLM output: {text[:60]}\"\n",
    "                break\n",
    "    state[\"route\"] = route\n",
    "    state[\"reasoning\"] = reasoning\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent Nodes\n",
    "\n",
    "Each of these functions represents a node in our graph. When the graph transitions to one of these nodes, the corresponding function is executed. Each function calls its specialized mock LLM with the query, and updates the `response` field in the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.779430Z",
     "start_time": "2025-09-09T00:16:43.777189Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_code_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process query with code-specialized LLM\"\"\"\n",
    "    response = code_llm(state['query'])\n",
    "    state['response'] = response\n",
    "    return state\n",
    "\n",
    "def handle_math_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process query with math-specialized LLM\"\"\"\n",
    "    response = math_llm(state['query'])\n",
    "    state['response'] = response\n",
    "    return state\n",
    "\n",
    "def handle_creative_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process query with creative-specialized LLM\"\"\"\n",
    "    response = creative_llm(state['query'])\n",
    "    state['response'] = response\n",
    "    return state\n",
    "\n",
    "def handle_general_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process query with general LLM\"\"\"\n",
    "    response = general_llm(state['query'])\n",
    "    state['response'] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph Construction\n",
    "\n",
    "Now we build the graph using `StateGraph`.\n",
    "\n",
    "1.  **Nodes**: We add the `router` and each of the specialized handler functions as nodes in the graph.\n",
    "2.  **Entry Point**: We set the `router` node as the entry point. All queries will start here.\n",
    "3.  **Conditional Edges**: After the `router` node, we use `add_conditional_edges`. The `determine_next_node` function reads the `route` from the state and tells the graph which specialized node to go to next.\n",
    "4.  **End Points**: After each specialized node has done its work, it transitions to the `END` state, finishing the execution for that query.\n",
    "5.  **Compilation**: Finally, we compile the workflow into a runnable application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.785202Z",
     "start_time": "2025-09-09T00:16:43.782832Z"
    }
   },
   "outputs": [],
   "source": [
    "def determine_next_node(state: AgentState) -> Literal['code', 'math', 'creative', 'general']:\n",
    "    \"\"\"Return the next node based on routing decision\"\"\"\n",
    "    return state['route']\n",
    "\n",
    "def create_routing_agent(use_llm: bool = True):\n",
    "    \"\"\"Create and return the LangGraph routing agent.\n",
    "\n",
    "    Args:\n",
    "        use_llm: If True and LLM available, use LLM router; otherwise keyword router.\n",
    "    \"\"\"\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # Choose router function\n",
    "    router_fn = llm_route_query if use_llm and ChatOpenAI else route_query\n",
    "\n",
    "    # Add nodes\n",
    "    workflow.add_node('router', router_fn)\n",
    "    workflow.add_node('code', handle_code_query)\n",
    "    workflow.add_node('math', handle_math_query)\n",
    "    workflow.add_node('creative', handle_creative_query)\n",
    "    workflow.add_node('general', handle_general_query)\n",
    "\n",
    "    workflow.set_entry_point('router')\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        'router', determine_next_node, {'code': 'code', 'math': 'math', 'creative': 'creative', 'general': 'general'}\n",
    "    )\n",
    "\n",
    "    workflow.add_edge('code', END)\n",
    "    workflow.add_edge('math', END)\n",
    "    workflow.add_edge('creative', END)\n",
    "    workflow.add_edge('general', END)\n",
    "\n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execution and Demonstration\n",
    "\n",
    "Below we instantiate the routing agent. If an OpenAI-compatible API key is present (OPENAI_API_KEY), the router uses an LLM to classify the query; otherwise it falls back to deterministic keyword routing. You can override the model via the `ROUTER_MODEL` environment variable. Set `use_llm=False` in `create_routing_agent` to force keyword routing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:43.904014Z",
     "start_time": "2025-09-09T00:16:43.788097Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = create_routing_agent(use_llm=True)\n",
    "print('Subgraph structure (router uses LLM =', bool(ChatOpenAI), ')')\n",
    "# Optional: Display a visualization of the graph's structure.\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T00:16:46.057193Z",
     "start_time": "2025-09-09T00:16:43.913966Z"
    }
   },
   "outputs": [],
   "source": [
    "from rich.panel import Panel\n",
    "from rich.rule import Rule\n",
    "\n",
    "test_queries = [\n",
    "    'How do I implement a binary search algorithm in Python?',\n",
    "    \"What's the integral of 2x³ + 5x² - 3x + 7?\",\n",
    "    'Write a mysterious short story about a lighthouse keeper',\n",
    "    'What causes the northern lights phenomenon?',\n",
    "    'Help me optimize this SQL query performance',\n",
    "    'Solve for x: 3x² - 12x + 9 = 0',\n",
    "]\n",
    "\n",
    "print(Panel.fit(\"[bold green]LangGraph LLM-Powered Routing Agent Demo[/bold green]\", style=\"bold\"))\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(Rule(f\"[bold]Query {i}[/bold]\", style=\"red\"))\n",
    "    print(f\"[bold]Query:[/bold] [italic]{query}[/italic]\\n\")\n",
    "\n",
    "    # Run the agent\n",
    "    result = agent.invoke({'query': query})\n",
    "\n",
    "    # Print results with rich formatting\n",
    "    print(f\"   [bold cyan]Route:[/bold cyan] [yellow]{result['route']}[/yellow]\")\n",
    "    print(f\"   [bold cyan]Reasoning:[/bold cyan] {result['reasoning']}\")\n",
    "    print(f\"   [bold cyan]Response:[/bold cyan] {result['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
