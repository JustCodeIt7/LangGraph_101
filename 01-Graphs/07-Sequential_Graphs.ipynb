{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0c022a",
   "metadata": {},
   "source": [
    "# Sequential Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502bdad",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to build sequential graphs for chatbot workflows using LangGraph and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babebd25",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88afbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "from rich import print\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3321f2",
   "metadata": {},
   "source": [
    "## Basic Chatbot Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28af0f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\")\n",
    "llm.invoke(\"Hello, world!\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a7ab7",
   "metadata": {},
   "source": [
    "### Agent State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9baf451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    name: str\n",
    "    age: str\n",
    "    final: str\n",
    "    message: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6002fbe",
   "metadata": {},
   "source": [
    "### Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906130c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greating_message(state: AgentState) -> AgentState:\n",
    "    \"\"\"Function to greet the user and initialize the state.\"\"\"\n",
    "    print(\"Welcome to the chatbot!\")\n",
    "    state['name'] = input(\"What is your name? \")\n",
    "    state['age'] = input(\"How old are you? \")\n",
    "    return state\n",
    "\n",
    "def chatbot(state: AgentState) -> AgentState:\n",
    "    \"\"\"Chatbot function that updates the state with user input.\"\"\"\n",
    "    user_input = input(\"You: \")\n",
    "    state['message'] = llm.invoke(user_input).content\n",
    "    print(f\"Chatbot: {state['message']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59afd2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469eb16",
   "metadata": {},
   "source": [
    "## Sequential Graph with LangGraph and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9f4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_chatbot.py\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b192ea",
   "metadata": {},
   "source": [
    "### 1. Define the Shared State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b861c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    name: str\n",
    "    user_question: str\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677432a",
   "metadata": {},
   "source": [
    "### 2. Node 1: Greet the User by Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5372d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(state: State) -> dict:\n",
    "    greeting = AIMessage(f\"Hi {state['name']}!\")\n",
    "    # Append to existing messages\n",
    "    return {\"messages\": state.get(\"messages\", []) + [greeting]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc15b7",
   "metadata": {},
   "source": [
    "### 3. Node 2: Take the User's Question and Get an LLM Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(state: State) -> dict:\n",
    "    # Build the message history: include the user's question\n",
    "    history = state[\"messages\"] + [HumanMessage(state[\"user_question\"])]\n",
    "    # Call the chat model\n",
    "    model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    response: AIMessage = model(history)\n",
    "    return {\"messages\": history + [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863208ab",
   "metadata": {},
   "source": [
    "### 4. Assemble the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e85a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(greet)\n",
    "builder.add_node(answer)\n",
    "# Define control flow: START → greet → answer → END\n",
    "builder.add_edge(START, \"greet\")\n",
    "builder.add_edge(\"greet\", \"answer\")\n",
    "builder.add_edge(\"answer\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c65839",
   "metadata": {},
   "source": [
    "### 5. Invoke the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff0b73ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_90597/927858589.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      "/var/folders/m7/yjmck8kn59gc9w3kdklj2lt40000gn/T/ipykernel_90597/927858589.py:6: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response: AIMessage = model(history)\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"user_question\": \"What's the capital of France?\",\n",
    "    \"messages\": []\n",
    "}\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e710de37",
   "metadata": {},
   "source": [
    "### 6. Print Out the Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4fdc1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Alice!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the capital of France?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
