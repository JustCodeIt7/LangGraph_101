# main.py

import os
from typing import TypedDict, Annotated, List
import operator
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, END
import uuid

# --- Environment Setup ---
# Set your Ollama model name. The user mentioned a default of llama3.2.
# Ensure Ollama is running on your local machine.
# You can run it with: `ollama run llama3.2`
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_API_KEY"] = "YOUR_LANGCHAIN_API_KEY" # Optional: For LangSmith tracing

# --- State Definition ---
# This TypedDict defines the structure of our application's state.
# It's how data is passed between nodes in the graph.
class ResearchState(TypedDict):
    """
    Represents the state of our research workflow.

    Attributes:
        topic (str): The initial research topic provided by the user.
        plan (str): The research plan generated by the LLM.
        feedback (str): Feedback from the human reviewer.
        research_result (str): The final research output.
        messages (Annotated[List[BaseMessage], operator.add]):
            A list to accumulate messages for the conversation history.
    """
    topic: str
    plan: str
    feedback: str
    research_result: str
    messages: Annotated[List[BaseMessage], operator.add]


# --- Tool & Model Definition ---
# Define the LLM we'll use for our agent. Using the user's preferred model.
model = ChatOllama(model="llama3.2", temperature=0)

# --- Node Functions ---
# These are the functions that will be executed as nodes in our graph.

def generate_plan(state: ResearchState) -> ResearchState:
    """
    Node to generate the initial research plan based on the user's topic.

    Args:
        state (ResearchState): The current state of the graph.

    Returns:
        ResearchState: The updated state with the generated plan.
    """
    print("--- Generating Research Plan ---")
    topic = state["topic"]
    messages = [
        HumanMessage(
            content=(
                f"Please create a concise, step-by-step research plan for the following topic: '{topic}'. "
                "The plan should be clear and actionable. Do not perform the research, just create the plan."
            )
        )
    ]
    response = model.invoke(messages)
    plan = response.content
    print(f"Generated Plan:\n{plan}")
    return {"plan": plan, "messages": messages}

def execute_research(state: ResearchState) -> ResearchState:
    """
    Node to execute the research based on the approved plan.

    Args:
        state (ResearchState): The current state of the graph.

    Returns:
        ResearchState: The updated state with the research result.
    """
    print("--- Executing Research ---")
    plan = state["plan"]
    topic = state["topic"]
    messages = [
        HumanMessage(
            content=(
                f"Topic: {topic}\n\n"
                f"Please execute the following research plan and provide a detailed summary of the findings.\n\n"
                f"Plan:\n{plan}"
            )
        )
    ]
    response = model.invoke(messages)
    research_result = response.content
    print(f"Research Result:\n{research_result}")
    return {"research_result": research_result, "messages": messages}

def plan_revision_node(state: ResearchState) -> ResearchState:
    """
    Node to revise the research plan based on human feedback.

    Args:
        state (ResearchState): The current state of the graph.

    Returns:
        ResearchState: The updated state with the revised plan.
    """
    print("--- Revising Research Plan ---")
    topic = state["topic"]
    feedback = state["feedback"]
    plan = state["plan"]
    messages = [
        HumanMessage(
            content=(
                f"The user has provided feedback on the research plan for the topic: '{topic}'.\n\n"
                f"Original Plan:\n{plan}\n\n"
                f"User Feedback:\n'{feedback}'\n\n"
                "Please generate a new, revised research plan that incorporates this feedback. "
                "The new plan should be a complete, standalone plan."
            )
        )
    ]
    response = model.invoke(messages)
    revised_plan = response.content
    print(f"Revised Plan:\n{revised_plan}")
    return {"plan": revised_plan, "messages": messages}


# --- Conditional Edge Logic ---
# This function determines the next step after the plan is generated.
def should_revise_plan(state: ResearchState) -> str:
    """
    Determines whether to revise the plan based on human feedback or proceed.

    Args:
        state (ResearchState): The current state of the graph.

    Returns:
        str: "revise_plan" if feedback is provided, otherwise "execute_research".
    """
    print("--- Checking for Feedback ---")
    feedback = state.get("feedback", "").strip().lower()
    if feedback and feedback not in ["approve", "approved", "proceed", "yes", "y"]:
        print("Feedback received. Revising plan.")
        return "revise_plan"
    else:
        print("Plan approved. Proceeding to research.")
        return "execute_research"


# --- Graph Definition ---
# Here we define the structure of our state machine.

# 1. Initialize the StateGraph
workflow = StateGraph(ResearchState)

# 2. Add nodes to the graph
workflow.add_node("generate_plan", generate_plan)
workflow.add_node("execute_research", execute_research)
workflow.add_node("revise_plan", plan_revision_node)

# 3. Set the entry point for the graph
workflow.set_entry_point("generate_plan")

# 4. Add edges to define the flow
workflow.add_edge("revise_plan", "execute_research") # After revising, always try to execute next
workflow.add_edge("execute_research", END)

# 5. Add a conditional edge for the human-in-the-loop step
workflow.add_conditional_edges(
    "generate_plan",
    should_revise_plan,
    {
        "revise_plan": "revise_plan",
        "execute_research": "execute_research",
    },
)

# 6. Compile the graph into a runnable application
# We interrupt *after* the `generate_plan` node to allow for human review.
# This pauses the graph at the perfect point to inject feedback.
app = workflow.compile(interrupt_after=["generate_plan"])


# --- Main Application Logic ---
def run_application():
    """
    Main function to run the human-in-the-loop research application.
    """
    print("--- Human-in-the-Loop Research Agent ---")
    
    topic = input("Please enter the research topic: ")
    
    # Use a config to manage the state thread across calls. This is crucial for resuming.
    # Each run gets a unique ID.
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # The input for the graph. It will be updated with feedback.
    current_input = {"topic": topic}
    
    while True:
        # Start or resume the stream with the current input and the same thread ID
        stream_iterator = app.stream(current_input, config=config, stream_mode="values")
        
        # The stream yields the state at each interruption point.
        for step in stream_iterator:
            # The graph is paused after `generate_plan`. `step` is the current state.
            if "plan" in step and "research_result" not in step:
                current_plan = step.get("plan")
                print("\n--- HUMAN REVIEW REQUIRED ---")
                print("The following research plan has been generated:")
                print("--------------------------------------------------")
                print(current_plan)
                print("--------------------------------------------------")
                
                feedback = input(
                    "Please review the plan. Type 'approve' to continue, "
                    "or provide feedback for revision: "
                )
                
                # Set the input for the next iteration to be the feedback
                current_input = {"feedback": feedback}
                break # Break from the inner for-loop to re-call stream with feedback
            else:
                # If we are here, it means the graph has finished.
                # The final step contains the result.
                final_state = step
                print("\n--- Research Complete ---")
                print("Final Research Summary:")
                print("--------------------------------------------------")
                print(final_state.get('research_result'))
                print("--------------------------------------------------")
                return # Exit the function
        else:
            # This else belongs to the for-loop. It runs if the loop completes without `break`.
            # This means the stream finished, so we can exit the while loop.
            break


if __name__ == "__main__":
    run_application()
