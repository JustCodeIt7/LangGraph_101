# LangGraph Streaming Examples

This directory contains examples demonstrating different streaming capabilities in LangGraph. Streaming is key to building responsive applications, and LangGraph provides several ways to stream data during agent execution.

## Examples Overview

1. **Agent Progress Streaming** (`01-agent_progress_streaming.py`)
   - Demonstrates how to stream agent progress updates using `stream_mode="updates"`
   - Emits an event after every agent step (LLM node, Tool node, etc.)

2. **LLM Tokens Streaming** (`02-llm_tokens_streaming.py`)
   - Demonstrates how to stream tokens as they are produced by the language model using `stream_mode="messages"`
   - Provides a more interactive and responsive user experience

3. **Tool Updates Streaming** (`03-tool_updates_streaming.py`)
   - Demonstrates how to stream custom updates from tools during execution using `stream_mode="custom"`
   - Uses `get_stream_writer()` to emit custom data during tool execution

4. **Multiple Streaming Modes** (`04-multiple_streaming_modes.py`)
   - Demonstrates how to stream multiple types of data at the same time using `stream_mode=["updates", "messages", "custom"]`
   - Combines all streaming types for comprehensive real-time updates

5. **Asynchronous Streaming** (`05-async_streaming.py`)
   - Demonstrates how to use asynchronous streaming with `astream()`
   - Shows async versions of all streaming modes (updates, messages, custom, and multiple)
   - Useful for asynchronous applications like web servers

## Running the Examples

Each example is a standalone Python script that can be run directly:

```bash
# Run agent progress streaming example
python 01-agent_progress_streaming.py

# Run LLM tokens streaming example
python 02-llm_tokens_streaming.py

# Run tool updates streaming example
python 03-tool_updates_streaming.py

# Run multiple streaming modes example
python 04-multiple_streaming_modes.py

# Run asynchronous streaming example
python 05-async_streaming.py
```

## Requirements

These examples use:
- LangGraph for agent creation and streaming
- Rich for better console output
- A language model (default is "ollama:llama3.2", but you can replace it with your preferred model)

## Key Concepts

### Agent Progress Streaming
Stream updates after each node in the agent graph is executed:

```python
for chunk in agent.stream(input_data, stream_mode="updates"):
    # Process agent progress updates
```

### LLM Tokens Streaming
Stream tokens as they are generated by the language model:

```python
for token, metadata in agent.stream(input_data, stream_mode="messages"):
    # Process LLM tokens
```

### Tool Updates Streaming
Stream custom updates from tools during execution:

```python
# In your tool function
def my_tool():
    writer = get_stream_writer()
    writer("Custom update from tool")
    return result

# In your streaming code
for chunk in agent.stream(input_data, stream_mode="custom"):
    # Process custom tool updates
```

### Multiple Streaming Modes
Stream multiple types of data at the same time:

```python
for stream_mode, chunk in agent.stream(
    input_data, 
    stream_mode=["updates", "messages", "custom"]
):
    if stream_mode == "updates":
        # Process agent updates
    elif stream_mode == "messages":
        # Process LLM tokens
    elif stream_mode == "custom":
        # Process custom tool updates
```

### Asynchronous Streaming
Stream data asynchronously using `astream()`:

```python
async for chunk in agent.astream(input_data, stream_mode="updates"):
    # Process agent progress updates asynchronously

# Or with multiple modes
async for stream_mode, chunk in agent.astream(
    input_data, 
    stream_mode=["updates", "messages", "custom"]
):
    # Process different types of streaming data asynchronously
```

## Additional Resources

For more information on streaming in LangGraph, see the [official documentation](https://langchain-ai.github.io/langgraph/agents/streaming/).
