{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph: Multi-Agent Parallel Processing with Different LLMs\n",
    "\n",
    "This notebook demonstrates a powerful multi-agent design pattern where multiple agents work in parallel to accomplish different tasks, each leveraging a different Large Language Model (LLM) optimized for its specific function. This approach allows for a more efficient and cost-effective system by assigning the best model for each job.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Multi-Agent Architecture**: Instead of a single monolithic agent, we use a team of specialized agents. Each agent has a distinct role and is powered by a carefully selected LLM.\n",
    "- **Parallel Execution**: LangGraph's `StateGraph` allows these agents to run concurrently, significantly speeding up the overall process. The entry point is branched to all parallel agents, and they all proceed to the end node after completion.\n",
    "- **LLM Specialization**: We use different OpenAI models for different tasks:\n",
    "  - `gpt-4o`: A highly capable model for generating thoughtful and relevant questions.\n",
    "  - `gpt-4o-mini`: A creative and cost-effective model perfect for generating jokes.\n",
    "  - `gpt-3.5-turbo`: A balanced and fast model for identifying related topics.\n",
    "- **State Management**: The `AgentState` TypedDict is used to pass information between the agents. It acts as a shared memory space where each agent can read the initial topic and write its results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, we import the necessary libraries and load our environment variables. This includes setting up API keys for OpenAI and LangChain tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "langchain_api_key = os.getenv('LANGCHAIN_API_KEY')\n",
    "if langchain_api_key:\n",
    "    os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Graph State\n",
    "\n",
    "The `AgentState` is a `TypedDict` that defines the data structure for our graph. It holds the shared state that all agents can access and modify.\n",
    "\n",
    "- `topic`: The input topic that the agents will work on.\n",
    "- `questions`: A list to store the questions generated by the `question_agent`.\n",
    "- `jokes`: A list to store the jokes generated by the `joke_agent`.\n",
    "- `related_topics`: A list to store related topics from the `related_topics_agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    questions: List[str]\n",
    "    jokes: List[str]\n",
    "    related_topics: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing Specialized LLMs\n",
    "\n",
    "We initialize three different LLMs, each chosen for its suitability for a specific task. This is a key aspect of the multi-agent design pattern, allowing for optimization of both performance and cost.\n",
    "\n",
    "- `llm_questions` (`gpt-4o`): Chosen for its strong reasoning and generation capabilities to produce high-quality, relevant questions.\n",
    "- `llm_jokes` (`gpt-4o-mini`): A more creative and less expensive model, ideal for the lighthearted task of generating humor.\n",
    "- `llm_related` (`gpt-3.5-turbo`): A fast and balanced model, well-suited for identifying connections and related concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_questions = ChatOpenAI(model='gpt-4o', temperature=0.1)  # Most capable for thoughtful questions\n",
    "llm_jokes = ChatOpenAI(model='gpt-4o-mini', temperature=0.8)  # Creative for humor\n",
    "llm_related = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.3)  # Balanced for topic connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Generation Chains\n",
    "\n",
    "We create a helper function `create_generation_chain` to build the processing chains for our agents. Each chain consists of a prompt, an LLM, and a JSON output parser.\n",
    "\n",
    "- **Prompt**: The `ChatPromptTemplate` guides the LLM on what to generate.\n",
    "- **LLM**: The language model that processes the prompt.\n",
    "- **Parser**: The `JsonOutputParser` ensures the output is in a structured JSON format, making it easy to integrate back into our `AgentState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generation_chain(llm, prompt_template_str: str):\n",
    "    \"\"\"Creates a chain for generating content based on a topic.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template_str)\n",
    "    parser = JsonOutputParser()\n",
    "    return prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this helper function to create a specific chain for each agent, each with its own prompt and specialized LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_chain = create_generation_chain(\n",
    "    llm_questions,\n",
    "    'Generate 5 questions about the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")\n",
    "joke_chain = create_generation_chain(\n",
    "    llm_jokes,\n",
    "    'Generate 3 jokes about the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")\n",
    "related_topics_chain = create_generation_chain(\n",
    "    llm_related,\n",
    "    'Generate 4 related topics for the following topic: {topic}. Output as a JSON object with a single key \"output\" that contains a list of strings.',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining the Agent Nodes\n",
    "\n",
    "Each agent is defined as a node in the graph. These functions take the current state as input, invoke their respective generation chains, and return the results to be merged back into the state.\n",
    "\n",
    "- `question_agent`: Generates questions using `gpt-4o`.\n",
    "- `joke_agent`: Generates jokes using `gpt-4o-mini`.\n",
    "- `related_topics_agent`: Generates related topics using `gpt-3.5-turbo`.\n",
    "\n",
    "Each agent includes error handling to gracefully manage any issues during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_agent(state: AgentState):\n",
    "    \"\"\"Generates questions based on the topic using GPT-4o.\"\"\"\n",
    "    print('---GENERATING QUESTIONS (GPT-4o)---')\n",
    "    try:\n",
    "        result = question_chain.invoke({'topic': state['topic']})\n",
    "        return {'questions': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in question_agent: {e}')\n",
    "        return {'questions': [f'Error generating questions: {str(e)}']}\n",
    "\n",
    "def joke_agent(state: AgentState):\n",
    "    \"\"\"Generates jokes based on the topic using GPT-4o-mini.\"\"\"\n",
    "    print('---GENERATING JOKES (GPT-4o-mini)---')\n",
    "    try:\n",
    "        result = joke_chain.invoke({'topic': state['topic']})\n",
    "        return {'jokes': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in joke_agent: {e}')\n",
    "        return {'jokes': [f'Error generating jokes: {str(e)}']}\n",
    "\n",
    "def related_topics_agent(state: AgentState):\n",
    "    \"\"\"Generates related topics based on the topic using GPT-3.5-turbo.\"\"\"\n",
    "    print('---GENERATING RELATED TOPICS (GPT-3.5-turbo)---')\n",
    "    try:\n",
    "        result = related_topics_chain.invoke({'topic': state['topic']})\n",
    "        return {'related_topics': result['output']}\n",
    "    except Exception as e:\n",
    "        print(f'Error in related_topics_agent: {e}')\n",
    "        return {'related_topics': [f'Error generating related topics: {str(e)}']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building and Compiling the Graph\n",
    "\n",
    "Now we assemble the graph using `StateGraph`.\n",
    "\n",
    "1.  **Add Nodes**: We add each agent function as a node in the graph.\n",
    "2.  **Set Entry Point**: A key feature for parallel execution is setting multiple entry points. By calling `set_entry_point` for each agent, we instruct LangGraph to run them all concurrently as soon as the graph is invoked.\n",
    "3.  **Add Edges**: We define the graph's flow. Since all agents run in parallel and the process finishes after they are done, we add an edge from each agent node directly to `END`.\n",
    "4.  **Compile**: Finally, we compile the workflow into a runnable application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes for each agent\n",
    "workflow.add_node('question_agent', question_agent)\n",
    "workflow.add_node('joke_agent', joke_agent)\n",
    "workflow.add_node('related_topics_agent', related_topics_agent)\n",
    "\n",
    "# The entry point is the topic, which then triggers the parallel agents\n",
    "workflow.set_entry_point('question_agent')\n",
    "workflow.set_entry_point('joke_agent')\n",
    "workflow.set_entry_point('related_topics_agent')\n",
    "\n",
    "# All parallel agents lead to the end\n",
    "workflow.add_edge('question_agent', END)\n",
    "workflow.add_edge('joke_agent', END)\n",
    "workflow.add_edge('related_topics_agent', END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Graph\n",
    "\n",
    "Here is a visual representation of our parallel agent graph. As you can see, the graph starts and immediately branches out to the three agents, which run in parallel before reaching the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    # Generate the Mermaid diagram and save it as a PNG\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Error generating graph visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running the Graph\n",
    "\n",
    "With the graph compiled, we can now run it. We'll test it with a few different topics to see how the agents perform.\n",
    "\n",
    "For each topic:\n",
    "1.  We define an `initial_state` dictionary containing the topic.\n",
    "2.  We `invoke` the compiled app with this state.\n",
    "3.  The graph executes the three agents in parallel.\n",
    "4.  The final state, containing the results from all agents, is returned and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('🚀 LangGraph Multi-Agent Parallel Processing with Different LLMs')\n",
    "print('=' * 65)\n",
    "print('🤖 Question Agent: GPT-4o (Most capable for thoughtful questions)')\n",
    "print('😂 Joke Agent: GPT-4o-mini (Creative and cost-effective for humor)')\n",
    "print('🔗 Related Topics Agent: GPT-3.5-turbo (Balanced for connections)')\n",
    "print('=' * 65)\n",
    "\n",
    "# Test with multiple topics\n",
    "test_topics = ['Artificial Intelligence', 'Climate Change', 'Space Exploration']\n",
    "\n",
    "for topic in test_topics:\n",
    "    print(f'\\n🎯 Processing Topic: {topic}')\n",
    "    print('⚡ Running parallel agents with different LLMs...')\n",
    "\n",
    "    initial_state = {'topic': topic, 'questions': [], 'jokes': [], 'related_topics': []}\n",
    "\n",
    "    try:\n",
    "        final_state = app.invoke(initial_state)\n",
    "\n",
    "        print(f'\\n📊 RESULTS FOR: {final_state[\"topic\"].upper()}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        print(f'\\n🤔 QUESTIONS (Generated by GPT-4o):')\n",
    "        for i, q in enumerate(final_state['questions'], 1):\n",
    "            print(f'  {i}. {q}')\n",
    "\n",
    "        print(f'\\n😂 JOKES (Generated by GPT-4o-mini):')\n",
    "        for i, j in enumerate(final_state['jokes'], 1):\n",
    "            print(f'  {i}. {j}')\n",
    "\n",
    "        print(f'\\n🔗 RELATED TOPICS (Generated by GPT-3.5-turbo):')\n",
    "        for i, t in enumerate(final_state['related_topics'], 1):\n",
    "            print(f'  {i}. {t}')\n",
    "\n",
    "        print('\\n' + '=' * 65)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'❌ Error processing topic \"{topic}\": {str(e)}')\n",
    "\n",
    "print('\\n✅ Demo completed! Each agent used a different LLM model.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
